---
title: "Factor Analysis and the Exploratory Approach"
author: "Jinsong Chen"
subtitle: "EDUR7109 Factor Analysis Week 1-2"
output:
  bookdown::html_document2:
    # code_folding: hide
    df_print: kable
    number_sections: false
    # theme: spacelab
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
  bookdown::word_document2:
    df_print: kable
    number_sections: false
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
  bookdown::pdf_document2:
    df_print: kable
    number_sections: false
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
editor_options:
  chunk_output_type: console
---

```{css, echo=FALSE}

body {
  font-size: 2em;
}

```

```{r setup, include=F}
# if (!require(bookdown)) { install.packages("bookdown"); library(bookdown) }
library(DiagrammeR)
library(DiagrammeRsvg)
library(rsvg)
knitr::opts_chunk$set(echo = T) #print code by default
options(digits=3)
```


<body>

## **Introduction**

* In the social and behavioral sciences, as well as many other disciplines, researchers often encounter a large set of observations or scores for a group of people or objects.
  - The data can come from any sources of measurement instruments including scale, questionnaire, survey, assessment, and so on.

* One important question that researchers often want to ask is if the large set of data can be parsimoniously represented with some underlying constructs or patterns.
  - That is equivalent to ask if the variability and complexity of the observed variables can be explained by some simpler structure.

* The question goes beyond univariate statistics (e.g., mean, variance) and appeals to bivariate statistics (e.g., correlation, covariance). One simple way to answer the question is to visually inspect the associations among the observed variables through the covariance matrix.
  - Note: The covariance matrix becomes the correlation matrix when the all diagonal terms are one.

### Examples and challenges

* Followings are two ideal cases of correlation matrix for inspection (note that the upper and lower triangles of the matrix are symmetrical):

```{r echo=FALSE,echo=FALSE,cache=FALSE,message=FALSE,warning=FALSE}
library(kableExtra)
opts <- options(knitr.kable.NA = "")
J<-8
mr<-matrix(.64,J,J)
diag(mr)<-1
is.na(mr) <- upper.tri(mr)
colnames(mr)<-rownames(mr)<-paste0("y",c(1:J))

mr %>%
  kbl(caption = "Ideal correlation matrix, Case One",digits = 2) %>%
  kable_classic(full_width = F, html_font = "Cambria") #,font_size = 18
  
```

```{r echo=FALSE,message=FALSE,warning=FALSE}
library(kableExtra)
opts <- options(knitr.kable.NA = "")
J<-8
mr<-matrix(.81,J,J)
mr[5:J,1:(J/2)]=0
mr[6:J,5:(J-1)]=.49
diag(mr)<-1
is.na(mr) <- upper.tri(mr)
colnames(mr)<-rownames(mr)<-paste0("y",c(1:J))

mr %>%
  kbl(caption = "Ideal correlation matrix, Case Two",digits = 2)%>%
  kable_classic(full_width= F, html_font = "Cambria") 

```

* Under both cases, it's not difficult to perceive a clear pattern across the $8 \times (8-1)/2=28$ correlations. For Case One, there's clearly one underlying construct for all observed variables; for Case Two, one can see an underlying construct for the first four variables, with another construct for the last four variables.

* Unfortunately, understanding the structure underlying a set of empirical data is rarely as easy as understanding the two ideal cases above. Following is an example from empirical data with some underlying patterns not easy to perceive visually.

```{r echo=FALSE}

fn<-"FA01.csv"
# write.csv(dat,fn,row.names=F)
x<-read.csv(fn)
xr<-cor(x)
is.na(xr) <- upper.tri(xr)
xr %>%
  kbl(caption = "Correlation matrix based on empirical data",digits = 3)%>%
  kable_classic(full_width= F, html_font = "Cambria")

```

* In practice, item correlations can be even negative, and thus vary across a wide range. More importantly, with more observed variables, the number of correlations to inspect will increase sharply, together with the difficulty for visual inspection. For instance, there are $30 \times (30-1)/2=435$ different correlations for a questionnaire with $30$ items, which will be a daunting task for visual inspection.

### Definitions and context

* Factor analysis (FA) was developed to overcome such challenges. It refers to a set of multivariate statistical techniques designed to understand the variability and complexity of observed variables with underlying constructs and patterns that are more parsimonious and explicable. The constructs presumed to account for the structure of correlations among the observed variables are referred to as *factors*, or more precisely as *common factors*, which are one type of latent variable that cannot be directly observed.

* Thus, FA can be considered as one type of latent variable models (LVMs), which will be discussed in more details later. Suffice it to know that LVMs are psychometric or measurement models in which each observed variable is regarded as an imperfect measure of the latent variable, namely with measurement error.

* In a broader sense, FA belongs to the family of structural equation modeling (SEM), which provides a methodological framework to conduct quantitative research. As a research framework, it is relevant to not only data analysis, but also how the research can be designed and implemented.

* When researchers have no clear expectation or a priori hypothesis about the underlying constructs and pattern of the observed variables, exploratory factor analysis (EFA) should be conducted, which is also referred to as unrestricted factor analysis. When clear expectations or specific hypotheses are available, confirmatory factor analysis (CFA) should be adopted, which is also known as restricted factor analysis. 

### Major terminology

* Followings are some key terms and notations that will be widely used under the factor analysis context. The symbols represent individual variables or parameters used in the equations or diagrams whereas the matrix forms refer to a complete set of the variables or parameters in the model.

```{r echo=FALSE}
library(kableExtra)

df<-data.frame(
  c1=c("Factor","Item or Indicator","Loading","Covariance / correlation", "Error"),
  c2=c("$f$","$y$","$a$","$r$, $\\sigma$,$c$","$e$"),
  c3=c("F","Y","A","R,$\\Sigma$, C","E"),
  c4=c("$\\bigcirc$","$\\square$","$\\longrightarrow$","$\\longleftrightarrow$",""),
 c5=c("Latent variable, cause","Observed variable, effect","(Directional) causal relationship, parameter","(Undirectional) association, bivariate information, parameter","Residual, random error, disturbance")
  )
header<-c("Term", "Symbol", "Matrix Form", "Diagram", "Meaning")

df %>%
  kbl(booktabs = T,caption = "Key terms for factor analysis", col.names=header)  %>% 
  kable_classic(full_width = F, html_font = "Cambria") %>% #,font_size = 20
  # column_spec(2,italic = T) %>%
  column_spec(3, bold = T)

```

* Note: the terms *item* and *indicator* will be used interchangeably

## **Common Factor Model**

### General form and single-factor model

* The foundation of factor analysis can be understood through the common factor model. With a mechanism similar to linear regression, the observed variables $\mathbf{Y}$ (i.e., the items) are modeled as a linear combination of common factors $\boldsymbol{f}$, plus the intercepts $\boldsymbol{\mu}$ and error terms $\mathbf{E}$. The model can be expressed concisely in matrix form, as: 

$$
\mathbf{Y} =\mathbf{AF+E}.
(\#eq:fa-matrix)
$$
where the loading matrix $\mathbf{A}$ consisting of individual loadings $a_{jk}$ reflects the strength of association between the factors $\boldsymbol{f}$ and items $\mathbf{Y}$, and plays a role similar to the coefficient parameter in regression. 

* There's one important difference between factor analysis and (multivariate) regression: the predictors in regression are observed variables whereas the common factors are unobserved or latent variables needed to be **estimated** from the model.

* In the common factor model, the factors are the latent causes (i.e., independent variables) while the items are the observable effects (dependent variables or outcomes), and the model captures the causal relationships between the two. Assuming there's only one factor with $J$ items, the model can be simplified as:

$$
y_j = a_{j} f + \boldsymbol{e}_j, \text{ for } j = 1...J.
(\#eq:fa-single)
$$
* This one-factor model can be more intuitively illustrated in Figure 1 below. Note that the directions of the arrows reflect the causality between the factor and items. There's a loading parameter and error term for each item, suggesting that each item can be considered as an imperfect measure of the factor (i.e., with measurement error). In comparison, the error term under the classical test theory (i.e., $\mathbf{E}$ in $\mathbf{X}=\mathbf{T}+\mathbf{E}$) is on the test level.

```{r fig.align='center',echo=FALSE,cache=FALSE,message=FALSE,warning=FALSE,fig.cap="A one-factor model with eight items."}
# par(mar = c(.1, .1, .1, .1))
a<-"digraph FA {
graph [layout = neato,
       overlap = flase,
       outputorder = edgesfirst,
       splines=true]
node [shape = rectangle]
# edge [arrowhead=vee, arrowtail=vee]
# Items
y1 [pos = '-3,4!', label = <y<SUB>1</SUB>>]
y2 [pos = '-2,4!', label = <y<SUB>2</SUB>>]
y3[pos = '-1,4!', label = <y<SUB>3</SUB>>]
y4[pos = '0,4!', label = <y<SUB>4</SUB>>]
y5[pos = '1,4!', label = <y<SUB>5</SUB>>]
y6[pos = '2,4!', label = <y<SUB>6</SUB>>]
y7[pos = '3,4!', label = <y<SUB>7</SUB>>]
y8[pos = '4,4!', label = <y<SUB>8</SUB>>]

#Factors
F1[pos = '.5,1!', label = <f>,shape=circle]
F1->F1[dir=both,label=<c=1>, splines=curved,tailport = 's', headport = 's']

#Loadings
F1->y1 [label = <a<SUB>1</SUB>>]
F1->y2 [label = <a<SUB>2</SUB>>]
F1->y3 [label = <a<SUB>3</SUB>>]
F1->y4 [label = <a<SUB>4</SUB>>]
F1->y5 [label = <a<SUB>5</SUB>>]
F1->y6 [label = <a<SUB>6</SUB>>]
F1->y7 [label = <a<SUB>7</SUB>>]
F1->y8 [label = <a<SUB>8</SUB>>]

#Errors
y1e[pos = '-3,5!', label = <E<SUB>1</SUB>>,shape=none]
y2e [pos = '-2,5!', label = <E<SUB>2</SUB>>,shape=none]
y3e[pos = '-1,5!', label = <E<SUB>3</SUB>>,shape=none]
y4e[pos = '0,5!', label = <E<SUB>4</SUB>>,shape=none]
y5e[pos = '1,5!', label = <E<SUB>5</SUB>>,shape=none]
y6e[pos = '2,5!', label = <E<SUB>6</SUB>>,shape=none]
y7e[pos = '3,5!', label = <E<SUB>7</SUB>>,shape=none]
y8e[pos = '4,5!', label = <E<SUB>8</SUB>>,shape=none]
y1e->y1
y2e->y2
y3e->y3
y4e->y4
y5e->y5
y6e->y6
y7e->y7
y8e->y8

}"
grViz(a)
# b<-grViz(a)
# tmp<-capture.output(rsvg_png(charToRaw(export_svg(b)),'fig1_1.png'))

```
<!-- ![A one-factor model with eight items.](fig1_1.png) -->

### Assumptions and scale invariance

* Assumptions are necessary before Equation \@ref(eq:fa-matrix) is admissible. First, the scale and center of the factors need to be fixed, usually with a mean of zero and standard deviation (SD) of one (note that no assumption about the distribution of the factors or data is required at this moment). Second, the error terms for different items are assumed to be independent of each other with a mean of zero. More precisely, we have a $J \times J$ diagonal covariance matrix:

$$
Var(\mathbf{E}) = D = \begin{bmatrix}
   d_{11} &  &  &  \\
   0 & d_{22} &  &  \\
    \ldots \\
   0 & 0 & \ldots & d_{JJ} \\
 \end{bmatrix}_{(J \times J)}.
$$

* Third, the factors and errors are independent of each other. More precisely, one has $Cov(\boldsymbol{f},\mathbf{E})=0$.

An important feature of Equation \@ref(eq:fa-matrix) is scale invariant, which means that the model is unaffected by re-scaling the data. Specifically, consider $\mathbf{Y'} =\mathbf{AY+C}$, where $\mathbf{A}$ and $\mathbf{C}$ are diagonal matrices. It turns out that:

$$
\begin{align*}
\mathbf{Y'} & = \mathbf{AY+C}\\
& =\mathbf{A}(\mathbf{AF+E})+\mathbf{C}\\
& =(\mathbf{A}\mathbf{A} \mathbf{F} + \mathbf{A}\mathbf{E}\\
& = \mathbf{A'} \mathbf{F} + \mathbf{E'}
\end{align*},
(\#eq:fa-trans)
$$
which has the same form and factors as Equation \@ref(eq:fa-matrix) (note: $D'=\mathbf{A}D\mathbf{A}$). It implies that when the data is linearly transformed, researchers will obtain a solution with the same factors plus a similar transformation of the parameters, which is essentially the same model. One convenient solution is to standardize the raw data as $z$ or standard score, namely:
$$
z_{j} =\frac{y_j-\bar{y_j} }{\text{SD}(y_j)}
$$
for $j = 1...J$.

* When the data is standardized, researchers will obtain the standardized loading parameters, similar to standardized coefficient in regression. Standardization is usually more convenient for factor analysis, and thus the default setting under the exploratory approach. From now on, we will assume all $y$s are standardized unless noted otherwise.

### Multiple-factor model

* With the same sets of data, the complexity of the model increases sharply when the number of factors rise. Figure 2 illustrates a two-factor model. Note that the loading matrix is more complicated and the factorial correlation $c_{12}$ is of concern.

```{r fig.align='center',echo=FALSE,cache=FALSE,message=FALSE,warning=FALSE,fig.cap="A two-factor model with eight items."}

# par(mar = c(.1, .1, .1, .1))
a<-"digraph FA2 {
graph [layout = neato,
       overlap = flase,
       outputorder = edgesfirst,
       splines=true]
node [shape = rectangle]
# edge [arrowhead = vee]
# Items
y1 [pos = '-4.5,4!', label = <y<SUB>1</SUB>>]
y2 [pos = '-3,4!', label = <y<SUB>2</SUB>>]
y3[pos = '-1.5,4!', label = <y<SUB>3</SUB>>]
y4[pos = '0,4!', label = <y<SUB>4</SUB>>]
y5[pos = '1.5,4!', label = <y<SUB>5</SUB>>]
y6[pos = '3,4!', label = <y<SUB>6</SUB>>]
y7[pos = '4.5,4!', label = <y<SUB>7</SUB>>]
y8[pos = '6,4!', label = <y<SUB>8</SUB>>]

#Factors
F1[pos = '-1.5,-1!', label = <f<SUB>1</SUB>>,shape=circle]
F2[pos = '3,-1!', label = <f<SUB>2</SUB>>,shape=circle]
F1->F1[dir=both,label=<c<SUB>11</SUB>=1>,tailport = 'sw', headport = 'nw'] # &#x3D5
F2->F2[dir=both,label=<c<SUB>22</SUB>=1>,tailport = 'se', headport = 'ne']
F1->F2[dir=both,label=<c<SUB>12</SUB>>]
dummy [pos = '.75,-1!', height=1, label='', color=white]

#Loadings
F1->y1 [label = <a<SUB>11</SUB>>]
F1->y2 [label = <a<SUB>21</SUB>>]
F1->y3 [label = <a<SUB>31</SUB>>]
F1->y4 [label = <a<SUB>41</SUB>>]
F1->y5 [label = <a<SUB>51</SUB>>]
F1->y6 [label = <a<SUB>61</SUB>>]
F1->y7 [label = <a<SUB>71</SUB>>]
F1->y8 [label = <a<SUB>81</SUB>>]
F2->y1 [label = <a<SUB>12</SUB>>]
F2->y2 [label = <a<SUB>22</SUB>>]
F2->y3 [label = <a<SUB>32</SUB>>]
F2->y4 [label = <a<SUB>42</SUB>>]
F2->y5 [label = <a<SUB>52</SUB>>]
F2->y6 [label = <a<SUB>62</SUB>>]
F2->y7 [label = <a<SUB>72</SUB>>]
F2->y8 [label = <a<SUB>82</SUB>>]

#Errors
y1e[pos = '-4.5,5!', label = <E<SUB>1</SUB>>,shape=none]
y2e [pos = '-3,5!', label = <E<SUB>2</SUB>>,shape=none]
y3e[pos = '-1.5,5!', label = <E<SUB>3</SUB>>,shape=none]
y4e[pos = '0,5!', label = <E<SUB>4</SUB>>,shape=none]
y5e[pos = '1.5,5!', label = <E<SUB>5</SUB>>,shape=none]
y6e[pos = '3,5!', label = <E<SUB>6</SUB>>,shape=none]
y7e[pos = '4.5,5!', label = <E<SUB>7</SUB>>,shape=none]
y8e[pos = '6,5!', label = <E<SUB>8</SUB>>,shape=none]
y1e->y1
y2e->y2
y3e->y3
y4e->y4
y5e->y5
y6e->y6
y7e->y7
y8e->y8

}"
grViz(a)
# b<-grViz(a)
# tmp<-capture.output(rsvg_png(charToRaw(export_svg(b)),'fig1_2.png'))
```
<!-- ![A two-factor model with eight items.](fig1_2.png) -->


* In general, a $K$-factor model can be expressed as:

$$
\begin{aligned}
y_j & =a_{j1} {f}_1 + a_{j2} {f_2} + ... + a_{jK} {f}_K + \boldsymbol{e}_j \\
& = \sum_{k=1}^{K} a_{jk} f_{k} + \boldsymbol{e}_j
\end{aligned},
\text{ for } j = 1...J.
(\#eq:fa-general).
$$

* And there are $K \times (K-1)/2$ correlations between factors.

## **The Exploratory Approach**

* Under the exploratory approach, one has no a priori information regarding the model. Thus, all elements in the loading matrix are unknown and need to be estimated, although it might turn out that many of them are approximately zero and can be dropped out of the equation:

$$
\underset{J \times K}{\mathbf{A}} = \begin{bmatrix}
   a_{11} & a_{12} & \ldots & a_{1K} \\
   a_{21} & a_{22} & \ldots & a_{2K} \\
    \ldots \\
   a_{J1} & a_{J2} & \ldots & a_{JK} \\
 \end{bmatrix}_{(J \times K)}.
 (\#eq:mlambda)
$$
* Under the EFA context, three important questions are required to be answered in sequence:

  1. What is the correct number of factors $K$?
  1. To what extent the factors are related to each other (i.e., factor correlations) in case there are more than one?
  1. What are the loading values and pattern (i.e., what elements in loading matrix are approximately zero and can be omitted)?

* To address the first question, we will first constrain the factors as orthogonal or uncorrelated of each other, which will be relaxed later. More precisely, we assume $Var(\mathbf{F})=\textbf{I}$, where $\textbf{I}$ is an identity matrix with all diagonal terms as one and off-diagonal terms as zero (i.e., $c_{12}=0$ in Figure 2). Moreover, the means of the factors are assumed to be zero.

### Decomposition of (co)variance

* With the above assumptions, one can better understand the nature of FA by decomposing the covariance matrix of the observed variables, which is just a correlation matrix when the data are standardized:

$$
\begin{aligned}
\mathbf{R}=E(\mathbf{YY}^T) & = E[\mathbf{(AF+E)(AF+E)}^T]\\
& =E(\mathbf{AFF}^T\mathbf{A}^T + \mathbf{AFE}^T+\mathbf{EF}^T\mathbf{A}^T+\mathbf{EE}^T)\\
& = \mathbf{A}E(\mathbf{FF}^T)\mathbf{A}^T+\mathbf{A}E(\mathbf{FE}^T)+E(\mathbf{EF}^T)\mathbf{A}^T+E(\mathbf{EE}^T)\\
& = \mathbf{A}\textbf{I}\mathbf{A}^T+\mathbf{A0}+\mathbf{0A}^T+\mathbf{D}\\
& = \mathbf{AA^\text{T}+D}
\end{aligned}
(\#eq:dec-var),
$$
where $E$ is the expected value, and the superscript $^T$ denotes the transpose of the matrix. Equation \@ref(eq:dec-var) shows that (co)variation among a set of observed variables (i.e., items) reflects the influence of the common factors (i.e., through the loading matrix), as well as unexplained or item-specific variance. 

* An example with two factors and three items are illustrated below:


$$
 \begin{bmatrix}
   1 &  &  \\
   r_{21} & 1 &  \\
   r_{31} & r_{32} & 1 \\
 \end{bmatrix} 
 =
 \begin{bmatrix}
   a _{11}^{2}+a _{12}^{2}+d_{11} &  &  \\
   a_{21}a_{11}+a_{22}a_{12} & a _{21}^{2}+a _{22}^{2}+d_{22} &  \\
   a_{31}a_{11}+a_{32}a_{12} & a_{31}a_{21}+a_{32}a_{22} & a _{31}^{2}+a _{32}^{2}+d_{33} \\
 \end{bmatrix}
 (\#eq:dec-ex).
$$

* The off-diagonal elements (e.g., $r_{21}$) illustrates that two items are highly correlated only if both items load strongly on at least one factor at the same time, which is exactly what we expected under the common factor model. The element can be summarized as:

$$
r_{ij}=\sum_{k=1}^{K} a _{ik}a _{jk}
(\#eq:off-diag).
$$

* In comparison, the diagonal terms can be summarized as:

$$
1=\underbrace{\sum_{k=1}^{K} a _{jk}^{2}}+\underbrace{d _{jj}^{2}}. \\
\qquad common\ unique
(\#eq:dec-single)
$$

* It shows that variation in one item can be partitioned into the (row) sum of squared loadings or *common* variance and error or *unique* variance. Common variance reflects the shared influence of common factors on an item, or the total variance an item is explained by the factors, which is also known as *communality* (sometimes denoted as $h^2$).

* Note that the square of individual loading $a _{jk}^2$ reflects the contribution of specific factor to the item variance, and the sum of all squared loadings reflect the total variance all common factors accounted for.

* Unique variance, which is called an item's *uniqueness* (sometimes denoted as $u^2$), refers to the remaining unexplained variation that has the same interpretation as the familiar concept of *residual*, *random error*, or *disturbance*.

* That is, item variance represents a) reliable variation in the item that reflects latent causes, and b) random error due to unreliability of measurement, which is usually referred to as *measurement error*. Since the sum of the two is exactly one, the larger the communality that the factors can explain, the smaller the uniqueness due to measurement error and vice versa.

### Estimation and extraction methods

* There are a variety of estimation and extraction methods under EFA. Here a brief introduction is given to three methods that are commonly used: Principal axis factoring (PAF), maximum likelihood estimation (MLE), and principal component analysis (PCA). For all these methods, a specific number of factors are assumed and the loading parameters are estimated based on Equation \@ref(eq:dec-var).

### Principal axis factoring (PAF)

* PAF, which is also known as principal factor analysis, involves decomposition of the reduced correlation matrix $\mathbf{R_c}$, which is defined as:

$$
\mathbf{R_c \equiv R-D} \approx \mathbf{AA^\text{T}}
(\#eq:dec-paf).
$$

* It means that the diagonal elements of the correlation matrix are replaced with the communalities. The approximate sign (i.e, $\approx$) suggests that approximation rather than exact solution is sought. PAF aims to decompose $\mathbf{R_c}$ into specific eigenvalues and eigenvectors. More importantly, the $k$-th largest eigenvalue corresponds to the variance explained by the $k$-th factor, which can be exploited for factor extraction.

* During estimation, the diagonal elements of $\mathbf{R_c}$ are initialized with the squared multiple correlations (SMCs), which refer to the proportion of variance in one item that is accounted for by other items. The provisional $\mathbf{R_c}$ is decomposed to find its eigenvalues and eigenvectors, which will be transformed into the loading parameters. Then communalities will be computed and used as new diagonal elements, and $\mathbf{R_c}$ will be decomposed again. The above process will be iterated until the changes of the communalities are trivial and the estimation is said to have converged on a solution.

### Maximum likelihood estimtion (MLE)

* MLE is a model fitting procedure widely used in SEM. It relies on two key assumptions about the data:

1. The observed variables are based on a random sample drawn from some defined population;
2. The population has a multivariate normal distribution, implying each item has a normal distribution and all item-pair associations are linear.

* Under the assumptions, we need to distinguish between sample and correlation matrices. The former comes from the sample data while the latter is implied by model parameters based on Equation \@ref(eq:dec-var). MLE aims to minimize the discrepancy between the two matrices by choosing parameter (e.g., loading) values that maximize the likelihood function, which reflects the relative likelihood of the data given a set of parameter estimates.

* Compared to PAF, MLE requires the above assumptions and more stringent condition of model identification. In return, it offers the uncertainty of parameter estimates and different test statistics which provides another means for factor extraction. More details about MLE will be discussed in later chapters.

### Principal component analysis (PCA)

* To be sure, PCA is **not** based on the common factor model, and thus distinctive from factor analysis, although both aim to reduce the dimensionality of a set of data. Principal components are not latent variables, but weighted composites or linear transformation of the observed variables. PCA does not assume a measurement model with a meaningful latent structure for the data per se, but simply transform the original variables into a new set of orthogonal variables that retains the total amount of variance.

* On the other hand however, PCA can be used to identify the number of factors with prominent results under certain conditions. Moreover, it is similar to the PAF, aiming to decompose the correlation matrix into eigenvalues with descending order. There's one major difference: the original correlation matrix rather than the reduced one is used:

$$
\mathbf{R} \approx \mathbf{AA^\text{T}}
(\#eq:dec-pca).
$$

* As a result, the $k$-th largest eigenvalue corresponds to the variance explained by the $k$-th principal component. The iteration process is also similar to PAF (but the initial values and decomposition algorithm are different). Compared Equation \@ref(eq:dec-pca) with Equation \@ref(eq:dec-paf), one can find that the unique variance (i.e., measurement error) is ignored. Thus, when the communities are not close to one (i.e., the measurement errors are not small), PCA and PAF can produce substantially different results.


## **Conventional Factor Extraction**

* Determining the number of factors is the first and an important step in EFA, the significance and challenges of which are often overlooked. Under-extraction (under-estimating the number of factors) results in substantial error on all loadings and factorial correlations, irrespective of their weight in a correctly specified model. Items that load onto a factor not included in the model can falsely load on other factors, which can alter true loadings and obscure true factor structure.

* In contrast, over-extraction (overestimating the number of factors) can lead to factor splitting, such that items loaded onto one factor are split into multiple factors. It also results in less parsimonious and explicable models that include constructs with little to no theoretical value.

* There are a variety of rules or criteria to determine the number of factors, many of which rely on eigenvalues associated with specific factors. Empirically, the eigenvalue for the $k$-th factor equal to the $k$-th column sum of squared loadings $\sum_{j=1}^{J}a _{jk}^2$, which represents the variance explained by that factor.

* But the logics behind the rules often differ substantially, with likely different results. In this section, we briefly review some conventional procedures, together with their limitations. 

### Kaiser-Guttman Criterion

* Kaiser-Guttman Criterion (KGC) is also known as the *eigenvalues-greater-than-one rule*. It extracts all factors with corresponding eigenvalues greater than one, and is one of the most prominent heuristics to determine the number of factors to retain. The rationale behind this rule is that a factor should at least explain as much variance as a single item (note the item variance is always one when the data are standardized). It is commonly used and is the default in some statistics programs.

* Note that both the PAF (i.e., with $\mathbf{R_c}$) and PCA (i.e., with $\mathbf{R}$) methods can generate descending eigenvalues, but with different values (Table 5), and PCA is generally preferred, although not without controversy.

```{r echo=FALSE,cache=FALSE,message=FALSE,warning=FALSE}
library(psych)
library(kableExtra)
fn<-"FA01.csv"

x<-read.csv(fn)
J<-ncol(x)

pc<-pca(x, nfactors = J, rotate = "none")
paf<-fa(x,nfactors = 3,  rotate = "none", fm = "pa")

df<-rbind(pc$values,paf$values)
row.names(df)<-c("PCA","PAF")
colnames(df)<-c(1:J)

df %>%
  kbl(caption = "Eigenvalues for the data in Table 3",digits = 3) %>%
  kable_classic(full_width = F, html_font = "Cambria")

```

* However, because sampling error leads to eigenvalues that exceed one even in the absence of any factor, the KGC severely overestimates the number of factors especially when there are many items. Another disadvantage of this rule is that it is quite arbitrary. In my opinion, it's more like a necessary but not sufficient condition.

### Cattell’s Scree Test

* Cattell’s scree test, which is also known as the scree plot, is a graphical method based on the plot of the successive eigenvalues in descending order. The test is performed by searching for an elbow, a point at which the eigenvalues decrease abruptly or substantially. The test suggests extracting all factors up to the factor corresponding to the eigenvalue preceding the sharpest decline. Namely, the number of plotted points before the last drop is the number of factors to include in the model.

* Being a graphical approach, the method is subjective in nature (i.e., there is no clear definition of what constitutes an abrupt or substantial drop). Furthermore, scree plots can be ambiguous, either lacking any clear elbow or showing multiple elbows in the same scree plot. The good news is that the test usually gives similar results using either the PCA or PAF methods, since it focuses on the descending trend, rather than the magnitude of the eigenvalues, as shown below (same data in Table 3):

```{r echo=FALSE}

scree(x,factors=T,pc=T)

```

### Parallel Analysis (PA)

* PA compares the empirical eigenvalues with the mean of eigenvalues obtained from random samples based on uncorrelated variables (with or without a scree plot). The random samples have the same number of observations and variables as the empirical data, so the eigenvalues of the random samples take sampling error into account. PA extracts all factors with eigenvalues that exceed the average corresponding eigenvalue of the random samples. Namely, the number of eigenvalues before the intersection points indicates how many factors to retain.

* The idea is that meaningful factors in the data should have eigenvalues larger than would be expected by chance. The eigenvalues in PA are typically based on PCA (i.e., with $\mathbf{R}$), although there's some debate if PAF (i.e., with $\mathbf{R_c}$) is more appropriate for a common factor model. Here's a PA scree plot based on the same data in Table 3:

```{r  results='hide',echo=FALSE,cache=FALSE,message=FALSE,warning=FALSE}
#library(psych)

fa.parallel(cor(x),n.iter=100,n.obs=nrow(x), fa="pc", fm="pa")

```

* One weakness associated with PA stems from the fact that sampling error can lead to eigenvalues above the average eigenvalue of random samples. For example, if all observed variables are uncorrelated such that there is no common factor, the first empirical eigenvalue would still exceed the first average eigenvalue from random samples with approximately $50$% of chance, which would lead to overestimation of the number of factors. Moreover, the method is very sensitive to sample size, and tends to suggest more factors with larger sample sizes.


***

## **Advanced Factor Extraction**

* Determining the number of factors is the first and an important step in EFA, the significance and challenges of which are often overlooked. Since conventional extraction methods often fail to do a good job, here some more advanced methods are offered as alternative, all of which are suggested by Auerswald and Moshagen (2019). R code to implement these methods are provided in exercise.

### Empirical Kaiser Criterion

* The Empirical Kaiser Criterion (EKC; Braeken & van Assen, 2017) is an approach that incorporates random sample variations of the eigenvalues in Kaiser's criterion. On a population level, the criterion is equivalent to Kaiser's criterion and extracts all factors with associated eigenvalues of the correlation matrix greater than one. However, on a sample level, the criterion takes the distribution of eigenvalues for normally distributed data into account.

* Under the null model, the distribution of eigenvalues asymptotically follows a Marcenko-Pastur distribution (Marcenko & Pastur, 1967). The resulting upper bound of this distribution is the reference value for the first eigenvalue. Subsequent eigenvalues are corrected by the explained variance, expressed as the eigenvalues of
previous factors. In accordance with the original Kaiser criterion, the reference eigenvalue cannot become smaller than one.

* Braeken and van Assen (2017) found that the EKC outperformed traditional PA method when factors are correlated and are only measured by few items with very high loadings. Furthermore, the EKC yielded comparable results to other advanced methods when there are a large number of factors and few observed variables.


### Sequential $\chi^2$ Model Tests

* The fit of common factor models can be assessed with the likelihood ratio test (LRT) statistic using maximum likelihood estimation, which tests whether the model-implied covariance matrix is equal to the population covariance matrix.

* The associated test statistic asymptotically follows a $\chi_2$ distribution if the observed variables follow a multivariate normal distribution and other assumptions are met (e.g., Bollen, 1989). This test can be sequentially applied to factor models with increasing numbers of factors, starting with a zero-factor model.
  - EFA models with increasing numbers of factors are hierarchical or nested with each other

* If the $\chi_2$ test statistic is statistically significant (with e.g., $p < .05$), a model with one additional factor, in this case a unidimensional factor model, is estimated and tested. The procedure continues until a nonsignificant result is obtained, at which point the number of common factors is identified.

* Simulation studies investigating the performance of sequential $\chi_2$ model tests (SMT) as an extraction criterion have shown conflicting results. Whereas some studies have shown that SMT has a tendency to overextract, others have indicated that the SMT has a tendency to underextract.

* Hayashi, Bentler, and Yuan (2007) demonstrated that overextraction tendencies are due to violations of regularity assumptions if the number of factors for the test exceeds the true number of factors.
  - For example, if a test of three factors is applied to samples from a population with two underlying factors, the LRT statistic will no longer follow a $\chi_2$ distribution. Note that the tests are applied sequentially, so a three-factor test is only employed if the two-factor test was incorrectly significant.
  - Therefore, this violation of regularity assumptions does not decrease the accuracy of SMT, but leads to (further) overextractions if a previous test was erroneously significant. 

### Different PA Variants

* Parallel Analysis (PA) is supported by strong evidence from simulation studies, and is generally considered to be the method of choice.

* As mentioned, PA compares the empirical eigenvalues with the mean of eigenvalues obtained from random samples based on uncorrelated variables (with or without a scree plot).
  - The random samples have the same number of observations and variables as the empirical data, so the eigenvalues of the random samples take sampling error into account.
  
* The eigenvalues in PA are typically based on the correlation matrix $\mathbf{R}$ of observed and random samples, similar to a PCA ($PA_{PCA}$).
  - It can also be based on the reduced correlation matrix $\mathbf{R_c}$ with communalities on the diagonal, reflecting the EFA (PAF) eigenvalues ($PA_{EFA}$)

* Moreover, the mean of eigenvalues obtained from random samples can be replaced with the 95th percentile of the eigenvalues, resulting in four PA versions
  - $PA_{PCA-M}$,$PA_{PCA-95}$,$PA_{EFA-M}$,$PA_{EFA-95}$

* In general, the PCA-based PA versions seem to produce better results than the EFA-based versions. But there's a weakness of all PA variants involves the choice of the reference eigenvalues for the second and following factors:
  - Assume that the empirical data set has a single underlying factor that explains a large portion of the item covariances.
  - However, the items in the random samples that constitute the comparison threshold are uncorrelated, leading to a higher portion of unexplained covariance for the random samples compared to the empirical sample.
  - This biased comparison, created by differing portions of unexplained covariance, might leave a second factor undetected and underestimate the number of factors.


### Combination Rules

* As shown, there's a drawback for any single method. Indeed, performance increased considerably when multiple factor extraction criteria were used simultaneously.
  - SMT performed especially well in conditions with highly correlated factor models but tended to overextract in the presence of moderate minor factors and displayed lower performance when data were not normally distributed.
  - As such, a useful complement to SMT would be an extraction criterion that does not tend to overextract and is robust against both minor factors and non-normality.
  - When SMT and either $PA_{PCA-M}$, $PA_{PCA-95}$, or the EKC agree, the correct number of factors is consistently identified. 
  - $PA_{PCA-M}$ performed well when data were non-normal or based on minor factors, but was reported to overextract in previous simulation studies (Glorfeld, 1995).
  - Because $PA_{PCA-95}$ and the EKC were both robust and did not overextract, we recommend combination rules consisting of SMT and one of the aforementioned.

* When the results of this combination rule are inconclusive (i.e., inconsistent), $PA_{PCA-95}$ and the EKC performed comparatively well. However, disagreement also suggests that factors will be harder to detect, and the sample size should be increased to 500, at least.


## **Factor Rotation**

* Rotation can be defined as a linear transformation of the pattern of the factor loadings in order to improve interpretability or to obtain a simpler structure.

### Rotational indeterminacy

* Once the number of factors is determined, we will be able to obtain an analytic solution to the model with parameter estimates. However, there exist an infinite number of equivalent solutions to an EFA model with two or more factors. To see this, denote $\bf{G}$ as an $K \times K$ orthogonal matrix such that $\bf{GG}^T=\bf{G}^T\bf{G}=\bf{I}$. The FA model can be re-expressed as:

$$
\begin{align*}
\bf{Y} & =\mathbf{AF+E}\\
& =\mathbf{A}\bf{GG}^T \mathbf{F} + \mathbf{E}\\
& = \mathbf{A}^* \mathbf{F}^* + \mathbf{E}
\end{align*},
(\#eq:fa-rot)
$$
where $\bf{A}^*=\bf{A G}$ and $\mathbf{F}^*=\bf{G}\mathbf{F}$. Mathematically, the new and original solutions are completely equivalent and indistinguishable, including the assumptions satisfied, the total variance explained, and the common and unique variance of each item.

* $\bf{G}$ is usually referred as a rotation matrix, the possible number of values which can take is infinite, and Equation \@ref(eq:fa-rot) is termed rotational indeterminacy. Although additional mathematical constraints can be imposed on the model to obtain an unique solution, it's against the nature of EFA that there should not be any a priori hypothesis.

### Simple structure and geometric representation

* Thus, researchers are required to overcome the challenge and find a more meaningful solution. One important means is to rotate the factor loadings in multidimensional space to arrive at a solution with the best simple structure and interpretability, which can be illustrated with geometric representation. An initial solution for a two-factor model is shown in the following table:

```{r echo=FALSE}
library(psych)
library(kableExtra)

fn<-"FA02.csv"
dat<-read.csv(fn)
lam<-fa(dat,nfactors = 2,  rotate = "none", fm = "pa")$loadings
x<-lam[,1]
y<-lam[,2]

df<-data.frame(F1=x,F2=y)
colnames(df)<-c("Factor 1", "Factor 2")
df %>%
  kbl(caption = "Intial loading matrix",digits = 3) %>%
  kable_classic(full_width = F, position = "center",html_font = "Cambria")

```

* It can be conceptualized as a space representation with the common factors as axes and the loading values as the coordinates of the items:

```{r echo=FALSE, fig.align='center',fig.cap="Geometrical representation",fig.asp=1, message=FALSE, warning=FALSE}
# library(psych)

plot_2f<-function(x,y,xa='Factor 1',ya='Factor 2'){
  plot.new()
  plot.window(xlim=c(-1,1),ylim=c(-1,1))
  text(.2,.9,xa)
  text(-.9,.1,ya)
  points(x=x,y=y)
  axis(1,at=c(-1,1),pos=0)
  axis(2,at=c(-1,1),pos=0)
}

plot_2f(x,y)
```

* We can rotate the axes of the factors to get different solutions (i.e., loading values). A simple structure is achieved when most items load largely on one factor, which means that the loading values are either large (close to one) or small (close to zero).

* Note that for specific item, if a loading approach one, all other loadings would be close to zero when the communality is unchanged. If we define complex items as items load substantially on more than one factor, and the corresponding loadings are called cross-loadings, a simple structure is to eliminate such items as many as possible.

```{r echo=FALSE, fig.align='center', fig.cap="Geometrical representation with rotated axes", fig.asp=1, message=FALSE, warning=FALSE}
library(psych)
plot_2f<-function(x,y){
  plot.new()
  plot.window(xlim=c(-1,1),ylim=c(-1,1))
  text(.2,.9,"Factor 1")
  text(-.9,.1,"Factor 2")
  points(x=x,y=y)
  axis(1,at=c(-1,1),pos=0)
  axis(2,at=c(-1,1),pos=0)
}

plot_2f(x,y)

# ind1<-which(y>0)
# ind2<-which(y<0)
# p1<-colMeans(lam[ind1,])
# p2<-colMeans(lam[ind2,])

# abline(0,p1[2]/p1[1],lty = 2)
# abline(0,p2[2]/p2[1],lty = 2)

abline(0,sqrt(3)/3,lty = 3,col='blue')
abline(0,-sqrt(3),lty = 3,col='blue')

# plot_2f(x,y)

abline(0,1,lty = 5,col='red')
abline(0,-1,lty = 5,col='red')

```

* The simple structure means that the item variance will be largely explained by one factor, or there's only one major cause (i.e., factor) underlying the item. This is the best simple interpretation one can find to explain the observed variable.

* Note that rotation does not change the total amount of variance explained (i.e., the sum of squared loadings) $-$ otherwise, it will give a different solution mathematically. Rather, it redistributes the variance and changes the eigenvalues across factors to aid in interpretation.

* There are two main types of factor rotation: orthogonal and oblique rotation, each with different rotational methods.

### Orthogonal rotation

* Remember that we assume the factors are orthogonal or uncorrelated. The assumption will be maintained under orthogonal rotation. *Varimax* is is the most common type of orthogonal rotation. It maximizes the variance of the squared loadings within a factor. Each factor will tend to have either large or small loadings. It tends to produce homogeneous factors that are more equitable, and yields result that is as easy as possible to identify each item with a single factor (i.e., simple structure). Below is the Varimax results for Table 1.

```{r echo=FALSE}

lam1<-fa(dat,nfactors = 2,  rotate = "varimax", fm = "pa")$loadings
x<-lam1[,1]
y<-lam1[,2]

df<-data.frame(F1=x,F2=y)
colnames(df)<-c("Factor 1", "Factor 2")
df %>%
  kbl(caption = "Loading matrix after varimax rotation",digits = 3) %>%
  kable_classic(full_width = F, position = "center",html_font = "Cambria")

```

```{r echo=FALSE, fig.align='center', fig.cap="Orthogonal rotation using varimax", fig.asp=1, message=FALSE, warning=FALSE}

plot_2f(x,y)

# abline(0,1,lty = 5,col='red')
# abline(0,-1,lty = 5,col='red')

```

* *Quartimax* is another orthogonal rotation that maximizes the squared loadings for each item rather than each factor. This minimizes the number of factors needed to explain each item, and often generates a general factor on which most items are loaded to a medium to high degree, plus other "small" factors. *Equimax* rotation is a compromise between varimax and quartimax.

### Oblique rotation

* It's often unrealistic to assume the factors are orthogonal. Oblique rotations permit correlations among factors, and thus can produce solutions with better simple structure when factors are expected to correlate. It also produces estimates of correlations among factors. Moreover, these rotations produce solutions similar to orthogonal rotation if the factors are uncorrelated.

* Accordingly, oblique rotation can be considered as a more general form of rotation that allows for both correlated and uncorrelated factors. Following is an example that oblique rotation is more appropriate (the blue lines are orthogonal).

```{r echo=FALSE, fig.align='center', fig.cap="Oblique factors example", fig.asp=1, message=FALSE, warning=FALSE}

fn<-"FA01.csv"
dat<-read.csv(fn)
lam<-fa(dat,nfactors = 2,  rotate = "none", fm = "pa")$loadings
x<-lam[,1]
y<-lam[,2]

plot_2f(x,y)

ind1<-which(y>0)
ind2<-which(y<0)
p1<-colMeans(lam[ind1,])
p2<-colMeans(lam[ind2,])

abline(0,p1[2]/p1[1],lty = 5,col='red')
abline(0,p2[2]/p2[1],lty = 5,col='red')

abline(0,1,lty = 3,col='blue')
abline(0,-1,lty = 3,col='blue')

```

* Direct oblimin rotation is the standard oblique rotation method. It tries to locate axes on the item clusters to produce smallest number of cross-loading or complex items (i.e., items load on more than one factor substantially). The level of obliqueness among factors is specified with a parameter $\delta$. Its value is usually set to $0$, which indicates an equal weighting of correlated versus uncorrelated factors and is also known as direct quartimin rotation.

* Promax rotation is often seen in older literature because it is easier to calculate than oblimin. It begins by conducting an initial Varimax rotation; after that, the factor loadings are raised to a power $\kappa$ with a mathematical transformation to drive down small loadings. High $\kappa$ value produces greater factorial correlations, and the default value of four is most commonly used.

## **Interpretation and Implementation**

### Interpreting factors

* After determining the number of factors and rotation, the factors can be interpreted and operationally defined with the loaded items under a simple structure. Researchers should examine the pattern of the rotated loading matrix to see which items load highly on which factors and then determine what those items have in common, which will indicate the meaning of the factors.

* For better interpretation, the loading pattern can be simplified based on a *cutoff*: (absolute) loading values smaller than the cutoff are considered as insignificant and dropped from the pattern and diagram. The cutoff is usually set as .3, which implies that there is no more than 10% variance (squared loading) explained by the factor.

* Following is the loading pattern after the oblimin rotation for the above oblique factors example, with the simple structure diagram. Note how the factors are defined by the items, the value of the factorial correlation, and the lack of complex item or cross-loading.

```{r echo=FALSE}
library(GPArotation)
opts <- options(knitr.kable.NA = "")

fn<-"FA01.csv"
dat<-read.csv(fn)
la0<-fa(dat,nfactors = 2,  rotate = "none", fm = "pa")$loadings
df0<-data.frame(la0[,1],la0[,2])
colnames(df0)<-c("Factor 1", "Factor 2")

m<-fa(dat,nfactors = 2,  rotate = "oblimin", fm = "pa")
lam<-m$loadings

df<-data.frame(F1=lam[,1],F2=lam[,2])
colnames(df)<-c("Factor 1", "Factor 2")
df1<-df
is.na(df1) <- (abs(df1) < .3) 

cbind(df0,df,df1) %>%
  kbl(caption = "Loading matrices before and after oblique rotation",digits = 3) %>%
  add_header_above(c(" ", "Initial Values" = 2, "Rotated" = 2, "After Cutoff" = 2)) %>%
  kable_classic(full_width = F, position = "center",html_font = "Cambria")

```


```{r fig.align='center',echo=FALSE,cache=FALSE,message=FALSE,warning=FALSE,fig.cap="A two-factor model after rotation."}
phi<-round(m$Phi[1,2],digits=2)
sla<-round(df1[!is.na(df1)],digits=2)
ind<-c(which(!is.na(df1[,1])),which(!is.na(df1[,2])))

# par(mar = c(.1, .1, .1, .1))
a<-("digraph FA2 {
graph [layout = neato,
       overlap = flase,
       outputorder = edgesfirst,
       splines=true]
node [shape = rectangle]
# edge [arrowhead = vee]
# Items
y1 [pos = '-4.5,4!', label = <y<SUB>@@3-1</SUB>>]
y2 [pos = '-3,4!', label = <y<SUB>@@3-2</SUB>>]
y3[pos = '-1.5,4!', label = <y<SUB>@@3-3</SUB>>]
y4[pos = '0,4!', label = <y<SUB>@@3-4</SUB>>]
y5[pos = '1.5,4!', label = <y<SUB>@@3-5</SUB>>]
y6[pos = '3,4!', label = <y<SUB>@@3-6</SUB>>]
y7[pos = '4.5,4!', label = <y<SUB>@@3-7</SUB>>]
y8[pos = '6,4!', label = <y<SUB>@@3-8</SUB>>]

#Factors
F1[pos = '-1.5,0!', label = <f<SUB>1</SUB>>,shape=circle]
F2[pos = '3,0!', label = <f<SUB>2</SUB>>,shape=circle]
F1->F1[dir=both,label=<c<SUB>11</SUB>=1>,tailport = 'sw', headport = 'nw'] # &#x3D5
F2->F2[dir=both,label=<c<SUB>22</SUB>=1>,tailport = 'se', headport = 'ne']
F1->F2[dir=both,label=<c<SUB>12</SUB>=@@1>]
dummy [pos = '.75,0!', height=1, label='', color=white]

#Loadings
F1->y1 [label = <a<SUB>11</SUB>=@@2-1>]
F1->y2 [label = <a<SUB>21</SUB>=@@2-2>]
F1->y3 [label = <a<SUB>41</SUB>=@@2-3>]
F1->y4 [label = <a<SUB>71</SUB>=@@2-4>]

F2->y5 [label = <a<SUB>32</SUB>=@@2-5>]
F2->y6 [label = <a<SUB>52</SUB>=@@2-6>]
F2->y7 [label = <a<SUB>62</SUB>=@@2-7>]
F2->y8 [label = <a<SUB>82</SUB>=@@2-8>]

#Errors
y1e[pos = '-4.5,5!', label = <E<SUB>@@3-1</SUB>>,shape=none]
y2e [pos = '-3,5!', label = <E<SUB>@@3-2</SUB>>,shape=none]
y3e[pos = '-1.5,5!', label = <E<SUB>@@3-3</SUB>>,shape=none]
y4e[pos = '0,5!', label = <E<SUB>@@3-4</SUB>>,shape=none]
y5e[pos = '1.5,5!', label = <E<SUB>@@3-5</SUB>>,shape=none]
y6e[pos = '3,5!', label = <E<SUB>@@3-6</SUB>>,shape=none]
y7e[pos = '4.5,5!', label = <E<SUB>@@3-7</SUB>>,shape=none]
y8e[pos = '6,5!', label = <E<SUB>@@3-8</SUB>>,shape=none]
y1e->y1
y2e->y2
y3e->y3
y4e->y4
y5e->y5
y6e->y6
y7e->y7
y8e->y8

}

[1]: phi
[2]: sla
[3]: ind

")
grViz(a)
# b<-grViz(a)
# tmp<-capture.output(rsvg_png(charToRaw(export_svg(b)),'fig1_2.png'))
```

### Interpreting the rotated loading matrix and factorial correlations

* With oblique or correlated factors, interpretation of item variance and communality is different. The correlation matrix becomes:

$$
\begin{aligned}
\mathbf{R}= \mathbf{ACA^\text{T}+D}
\end{aligned}
(\#eq:dec-rot),
$$

* For the two-factor model above, item communality is $a _{j1}^2+a _{j2}^2+2a _{j1}a _{j2}C_{12}$, which reduces to the sum of squared loadings when the factors are uncorrelated or there's cross-loading. In comparison, item-pair correlation becomes:
$$
r_{ij}= a _{i1}a _{j1}+a _{i2}a _{j2}+(a _{i1}a _{j2}+a _{i2}a _{j1})C_{12}
(\#eq:off-rot).
$$

### Implementation procedures

* The implementation of EFA is straightforward following the three questions:

  1. Determine the number of factors, preferably based on the combination rules. If the result is inconclusive or inconsistent, adjustment to the research is suggested (i.e., increase the sample size or add/remove some items);
  1. Conduct orthogonal or oblique rotation. If the MLE method is adopted, additional fit evaluation can be conducted using fit statistics;
  1. Ideally, a simple structure without complex item is obtained, and the factors can be interpreted based on the rotated loading matrix. If there are complex items and the number of items is large, one might consider to reanalyze without the complex items.


* Adjust items in case needed (may need to repeat Step 2 and 3)

### Model Evaluation and Utility

* A good EFA model: 
  - Makes sense: theoretically sound 
  - Is easy to interpret: the fewer factors the better
  - Reach similar result regardless of factor or loading cutoff criteria
  - Has a simple structure: lacks complex or multidimensional items measuring more than one factor (i.e., no cross-loading)
  
* In addition to function as a research method, EFA can be used for scale construction and item selection
  - Investigate scale dimensionality (i.e., if the number of factors is the same as expected)
  - Items with poor loadings are useless, and should be revised or dropped
  - Items with cross-loadings load substantially on more than one factor, which is usually not the intention of item design; it's generally best to avoid using such items in the official version of the scale

* However, factor score under EFA is not quite useful (and not suggested) due to rotation indeterminacy

### Criticism and Clarifications

* There are some common criticisms against EFA:
  - Exploratory means data-driven and to try everything possible, which capitalize on chance (e.g., # factors, rotations, cutoffs for loadings)
  - Different extraction methods used (PCA, PAF, ML, Scree Test, etc.) can result in different number of extracted factors
  -Parameter estimates can change substantially due to different rotation methods under either the Orthogonal or Oblique rotation
  - It's not subject to robust statistical test for evaluation of model-data fit, and there's no hypothesis available about the number or nature of the factors
  - Interpretation of results tend to be subjective, and results are not defensible without further proof (some researchers call EFA more of an art than a science)

* However, many of these criticisms just need clarifications:
  - EFA is only suggested when there's little literature or theoretical foundation, where there's not much one can do besides using EFA; in contrast, EFA should NOT be used when there is enough substantive or theoretical support in the research
  - The combination rule is suggested for factor extraction, and if the result is inconclusive or inconsistent, adjustment to the research is suggested
  - Oblique rotation is usually suggested (unless there's substantive rationale that the factors should be uncorrelated), and direct oblimin rotation is the method of choice
  - Statistical or hypothesis testing is less meaningful under the exploratory setting where there's little theoretical or substantive support a priori
  - researchers should be more prudent to interpret the findings, and confirmatory factor analysis (with a different set of data) is suggested to cross-validate the results


</body>
